{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Decision Tree | Assignment"
      ],
      "metadata": {
        "id": "d2lgYS0SZYvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 1: What is a Decision Tree, and how does it work in the context of classification?"
      ],
      "metadata": {
        "id": "6ZSlk1N6Zamg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4c84e56"
      },
      "source": [
        "A Decision Tree is a supervised machine learning algorithm that can be used for both classification and regression tasks. It's called a 'tree' because it uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility.\n",
        "\n",
        "### How it works in the context of classification:\n",
        "\n",
        "1.  **Splitting Criteria**: The algorithm starts at the 'root' node and splits the data into two or more homogeneous sets based on the feature that best separates the classes. The goal is to maximize the homogeneity within the resulting subsets and heterogeneity between them. Common splitting criteria include:\n",
        "    *   **Gini Impurity**: Measures how often a randomly chosen element from the set would be incorrectly labeled if it were randomly labeled according to the distribution of labels in the subset.\n",
        "    *   **Entropy/Information Gain**: Entropy measures the disorder or randomness of a set of examples. Information Gain is the reduction in entropy achieved by splitting the data on a particular feature.\n",
        "\n",
        "2.  **Recursive Splitting**: This splitting process is then recursively applied to each child node. The recursion stops when one of the following conditions is met:\n",
        "    *   All samples at a node belong to the same class.\n",
        "    *   There are no remaining features to split on.\n",
        "    *   A pre-defined stopping criterion is met (e.g., maximum tree depth, minimum number of samples per leaf node).\n",
        "\n",
        "3.  **Leaf Nodes**: The final nodes, where no further splitting occurs, are called 'leaf' nodes. Each leaf node represents a class label (in classification) or a predicted value (in regression).\n",
        "\n",
        "4.  **Prediction**: To classify a new data point, you traverse the tree from the root to a leaf node by following the decisions based on the data point's feature values. The class label of the leaf node is then assigned as the prediction for that data point.\n",
        "\n",
        "### Analogy:\n",
        "\n",
        "Imagine you're trying to decide what to do on a Saturday. You might first ask, \"Is it raining?\" If yes, you might then ask, \"Do I have an umbrella?\" If no, you might ask, \"Is it warm enough to go out without one?\" Each question is a decision node, and your answer leads you down a different branch of the tree until you reach a final activity (leaf node) like \"read a book\" or \"go for a walk.\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?"
      ],
      "metadata": {
        "id": "pTiHg4YWZeR5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6783870"
      },
      "source": [
        "Gini Impurity and Entropy are two common metrics used in Decision Trees to measure the 'impurity' or 'disorder' of a set of samples. The goal of a Decision Tree algorithm is to choose splits that maximize the reduction in impurity.\n",
        "\n",
        "### 1. Gini Impurity\n",
        "\n",
        "**Concept**: Gini Impurity measures how often a randomly chosen element from a set would be incorrectly labeled if it were randomly labeled according to the distribution of labels in the subset. It ranges from 0 to 0.5 (for a binary classification problem), where 0 indicates perfect purity (all samples belong to the same class) and 0.5 indicates maximum impurity (samples are equally distributed among classes).\n",
        "\n",
        "**Formula for a node `t`**: If `p_i` is the fraction of items with property `i` in the node, then:\n",
        "`Gini(t) = 1 - sum(p_i^2)` for all classes `i`\n",
        "\n",
        "**Impact on Splits**: When building a Decision Tree, the algorithm calculates the Gini impurity for each potential split. It then chooses the split that results in the lowest weighted average Gini impurity of the child nodes, or, equivalently, the split that provides the greatest \"Gini Gain\" (reduction in impurity).\n",
        "\n",
        "*   **Advantages**: Computationally less intensive than Entropy because it doesn't involve logarithms.\n",
        "*   **Disadvantages**: Tends to isolate the most frequent class in its own branch of the tree.\n",
        "\n",
        "### 2. Entropy\n",
        "\n",
        "**Concept**: Entropy is a measure of the disorder or randomness in a set of samples. In the context of classification, it quantifies the uncertainty associated with predicting the class of a new sample. Entropy ranges from 0 (perfect purity, all samples belong to the same class) to 1 (maximum impurity for binary classification, samples are equally distributed among classes).\n",
        "\n",
        "**Formula for a node `t`**: If `p_i` is the fraction of items with property `i` in the node, then:\n",
        "`Entropy(t) = -sum(p_i * log2(p_i))` for all classes `i`\n",
        "(Note: if `p_i` is 0, then `p_i * log2(p_i)` is taken as 0)\n",
        "\n",
        "**Impact on Splits**: Similar to Gini Impurity, the Decision Tree algorithm calculates the Entropy for each potential split. It aims to maximize \"Information Gain,\" which is the reduction in entropy achieved by splitting the data on a particular feature. The split that yields the highest information gain is chosen.\n",
        "\n",
        "*   **Advantages**: Provides a more balanced tree, as it tries to make the distributions in child nodes as distinct as possible.\n",
        "*   **Disadvantages**: Computationally more intensive due to the use of logarithms.\n",
        "\n",
        "### How they impact the splits in a Decision Tree:\n",
        "\n",
        "Both Gini Impurity and Entropy serve the same fundamental purpose: to find the best way to divide the data at each node. The algorithm iteratively selects the feature and the split point that minimizes the impurity (or maximizes the impurity reduction) in the resulting child nodes. This process continues recursively until a stopping criterion is met, leading to a tree where leaf nodes are as pure as possible, meaning they primarily contain samples of a single class."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each."
      ],
      "metadata": {
        "id": "L3RfZeMWZ3Gy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e83d57de"
      },
      "source": [
        "Pre-Pruning and Post-Pruning are techniques used to prevent overfitting in Decision Trees by controlling their complexity.\n",
        "\n",
        "### 1. Pre-Pruning (Early Stopping)\n",
        "\n",
        "**Concept**: Pre-pruning involves stopping the tree construction early, before it has fully grown. This is done by setting constraints or conditions that, if met, prevent further splitting of a node. The tree building process stops prematurely if these criteria are satisfied.\n",
        "\n",
        "**Common Pre-Pruning Criteria**:\n",
        "*   **Maximum Depth**: Limit the maximum depth of the tree.\n",
        "*   **Minimum Samples per Split**: Require a minimum number of samples in a node for it to be considered for splitting.\n",
        "*   **Minimum Samples per Leaf**: Require a minimum number of samples in any leaf node.\n",
        "*   **Maximum Features**: Limit the number of features considered for the best split.\n",
        "*   **Impurity Threshold**: Stop splitting if the impurity gain from a split is below a certain threshold.\n",
        "\n",
        "**Practical Advantage**: **Computational efficiency**. Pre-pruning is generally faster because it prevents the tree from growing unnecessarily large. This can save significant computation time, especially with large datasets, as the algorithm doesn't have to build and then later prune a very complex tree.\n",
        "\n",
        "### 2. Post-Pruning (Backward Pruning)\n",
        "\n",
        "**Concept**: Post-pruning involves first growing the Decision Tree to its full potential (or a very large size) without any stopping criteria other than perfect purity. After the full tree is built, subtrees or branches are then removed or collapsed (pruned) from the bottom-up, usually based on some error estimation or validation metric.\n",
        "\n",
        "**Common Post-Pruning Methods**:\n",
        "*   **Reduced Error Pruning**: Uses a separate validation set to evaluate the performance of pruning different subtrees. It prunes the subtree if the pruned tree performs better on the validation set.\n",
        "*   **Cost-Complexity Pruning (Weakest Link Pruning)**: Involves calculating a complexity parameter (alpha) for each node and iteratively removing the node that minimizes a cost function (usually a combination of error and tree complexity).\n",
        "\n",
        "**Practical Advantage**: **Potentially better accuracy**. Post-pruning can often lead to a more optimal or accurate tree because it considers all possible splits and branches before making pruning decisions. By growing the full tree first, it can capture more complex relationships that might be missed by early stopping criteria in pre-pruning, and then refine it based on performance metrics to find the best balance between bias and variance.\n",
        "\n",
        "### Key Differences Summary:\n",
        "\n",
        "| Feature           | Pre-Pruning                               | Post-Pruning                                |\n",
        "| :---------------- | :---------------------------------------- | :------------------------------------------ |\n",
        "| **When Applied**  | During tree construction                  | After the full tree is built                |\n",
        "| **Mechanism**     | Stops growth early based on criteria      | Grows full tree, then removes branches      |\n",
        "| **Complexity**    | Generally simpler, less complex trees     | Can explore more complex structures initially |\n",
        "| **Computational** | More efficient                            | Less efficient (builds full tree first)     |\n",
        "| **Risk**          | May stop too early (underfitting)         | May build overly complex tree initially (overfitting) |"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?"
      ],
      "metadata": {
        "id": "RwZkqPxSaA4C"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0657957"
      },
      "source": [
        "Information Gain is a key concept in the construction of Decision Trees, particularly when using entropy as an impurity measure. It quantifies the reduction in entropy (or uncertainty) achieved by splitting a dataset based on a particular feature.\n",
        "\n",
        "### What is Information Gain?\n",
        "\n",
        "At its core, Information Gain measures how much \"information\" a feature provides about the class labels. A higher information gain implies that the split using that feature does a better job of separating the data into more homogeneous (pure) groups with respect to the target variable.\n",
        "\n",
        "**Formula**:\n",
        "\n",
        "`Information Gain(S, A) = Entropy(S) - Σ [ ( |Sv| / |S| ) * Entropy(Sv) ]`\n",
        "\n",
        "Where:\n",
        "*   `S` is the set of all samples at the current node.\n",
        "*   `A` is the feature being considered for the split.\n",
        "*   `Entropy(S)` is the entropy of the original set `S`.\n",
        "*   `Sv` is the subset of `S` for which feature `A` has value `v`.\n",
        "*   `|Sv|` is the number of samples in `Sv`.\n",
        "*   `|S|` is the total number of samples in `S`.\n",
        "*   `Σ` is the summation over all possible values `v` of the feature `A`.\n",
        "\n",
        "In simpler terms, Information Gain is the entropy of the parent node minus the weighted average entropy of the child nodes created by the split.\n",
        "\n",
        "### Why is it important for choosing the best split?\n",
        "\n",
        "Information Gain is crucial because the primary objective of a Decision Tree algorithm is to create branches that lead to the purest possible leaf nodes. By selecting the feature that yields the highest Information Gain, the algorithm ensures that each split moves the tree closer to this goal.\n",
        "\n",
        "Here's why its importance:\n",
        "\n",
        "1.  **Maximizes Purity**: A split with high Information Gain effectively reduces the randomness or disorder within the resulting subsets. This means the child nodes become more homogeneous, making it easier to classify samples accurately.\n",
        "2.  **Guides Tree Construction**: At each node, the Decision Tree algorithm (like ID3 or C4.5) iterates through all available features and calculates the Information Gain for each potential split. The feature that provides the maximum Information Gain is chosen as the splitting criterion for that node.\n",
        "3.  **Reduces Decision Path Length**: By making optimal splits, Information Gain helps in creating a more concise and efficient tree. This means that fewer decisions are needed to classify a new instance, leading to faster predictions.\n",
        "4.  **Feature Selection**: Implicitly, Information Gain acts as a form of feature selection. Features that are highly correlated with the target variable and thus lead to significant reductions in entropy will be prioritized for splitting, effectively highlighting the most important features in the dataset.\n",
        "\n",
        "In essence, Information Gain provides a quantitative measure to determine which attribute in a dataset is most effective in classifying the training examples, thereby guiding the tree construction process towards an optimal and efficient structure."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?"
      ],
      "metadata": {
        "id": "jSxZ7242aNsB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e69df2ef"
      },
      "source": [
        "Decision Trees are a popular and interpretable machine learning algorithm, finding applications across various domains due to their simplicity and effectiveness. Here are some common real-world applications, along with their main advantages and limitations:\n",
        "\n",
        "### Common Real-World Applications:\n",
        "\n",
        "1.  **Medical Diagnosis**: Identifying potential diseases based on symptoms, patient history, and test results. For example, classifying patients as high-risk or low-risk for certain conditions.\n",
        "2.  **Credit Risk Assessment**: Banks and financial institutions use decision trees to evaluate the creditworthiness of loan applicants, predicting whether an applicant is likely to default on a loan based on factors like income, credit history, and employment status.\n",
        "3.  **Customer Relationship Management (CRM)**: Predicting customer churn (who is likely to leave a service), identifying potential customers for targeted marketing campaigns, or segmenting customers based on their behavior and demographics.\n",
        "4.  **Fraud Detection**: Detecting fraudulent transactions in banking, insurance, or e-commerce by identifying patterns that deviate from normal behavior.\n",
        "5.  **Manufacturing Quality Control**: Identifying defects in manufactured products based on various process parameters, helping to optimize production lines and reduce waste.\n",
        "6.  **Recommendation Systems**: Though often used as part of larger ensemble methods (like Random Forests), decision trees can help categorize users or items to provide personalized recommendations.\n",
        "7.  **Image Classification**: In some simpler cases, decision trees can be used to classify images, though deep learning methods are more prevalent for complex image tasks.\n",
        "8.  **Bioinformatics**: Classifying DNA sequences, protein structures, or predicting gene function.\n",
        "\n",
        "### Main Advantages:\n",
        "\n",
        "1.  **Easy to Understand and Interpret**: Decision trees mimic human decision-making, making them very intuitive and easy to explain to non-technical stakeholders. The tree structure can be visualized, providing clear insights into the decision process.\n",
        "2.  **No Data Preprocessing Required (or minimal)**: They can handle both numerical and categorical data without extensive preprocessing like normalization or scaling.\n",
        "3.  **Handles Missing Values**: They can effectively handle missing values, especially when using specific splitting criteria that account for them.\n",
        "4.  **Non-Linear Relationships**: Decision trees can capture non-linear relationships between features and the target variable.\n",
        "5.  **Feature Selection**: The tree-building process implicitly performs feature selection, as more important features tend to appear closer to the root of the tree.\n",
        "6.  **Versatility**: Can be used for both classification and regression tasks.\n",
        "\n",
        "### Main Limitations:\n",
        "\n",
        "1.  **Overfitting**: Decision trees are prone to overfitting, especially when they are allowed to grow too deep. This means they might perform very well on training data but poorly on unseen data. Pruning techniques are crucial to mitigate this.\n",
        "2.  **Instability**: Small changes in the data can lead to a completely different tree structure. This instability can make them less reliable when the data has high variance.\n",
        "3.  **Bias with Imbalanced Data**: If the dataset has imbalanced classes, decision trees can be biased towards the majority class. Techniques like re-sampling or cost-sensitive learning are often needed.\n",
        "4.  **Local Optimality**: The greedy approach of choosing the best split at each node does not guarantee a globally optimal tree. The algorithm might get stuck in a local optimum.\n",
        "5.  **Complexity for Large Trees**: While individual trees are easy to interpret, a very deep and complex tree can become difficult to understand and visualize.\n",
        "6.  **Limited Expressiveness for Continuous Variables**: When dealing with continuous variables, decision trees make splits by creating orthogonal boundaries, which might not be optimal for all types of data distributions."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 6: Write a Python program to:\n",
        "- Load the Iris Dataset\n",
        "- Train a Decision Tree Classifier using the Gini criterion\n",
        "- Print the model’s accuracy and feature importances"
      ],
      "metadata": {
        "id": "uTgh9at4adm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 2. Train a Decision Tree Classifier using the Gini criterion\n",
        "dt_classifier = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "\n",
        "# 3. Print the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# 4. Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, dt_classifier.feature_importances_):\n",
        "    print(f\"  {feature}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEXSi0bUaw8J",
        "outputId": "3878fa57-29d7-4bc4-cb86-8d556c4a5609"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n",
            "\n",
            "Feature Importances:\n",
            "  sepal length (cm): 0.0000\n",
            "  sepal width (cm): 0.0191\n",
            "  petal length (cm): 0.8933\n",
            "  petal width (cm): 0.0876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 7: Write a Python program to:\n",
        "- Load the Iris Dataset\n",
        "- Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree."
      ],
      "metadata": {
        "id": "OHsawCDPa9gJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# --- Model 1: Decision Tree with max_depth=3 ---\n",
        "dtc_max_depth_3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "dtc_max_depth_3.fit(X_train, y_train)\n",
        "\n",
        "y_pred_max_depth_3 = dtc_max_depth_3.predict(X_test)\n",
        "accuracy_max_depth_3 = accuracy_score(y_test, y_pred_max_depth_3)\n",
        "print(f\"Accuracy of Decision Tree (max_depth=3): {accuracy_max_depth_3:.2f}\")\n",
        "\n",
        "# --- Model 2: Fully-grown Decision Tree ---\n",
        "dtc_full = DecisionTreeClassifier(random_state=42) # No max_depth specified means fully grown\n",
        "dtc_full.fit(X_train, y_train)\n",
        "\n",
        "y_pred_full = dtc_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "print(f\"Accuracy of Fully-grown Decision Tree: {accuracy_full:.2f}\")\n",
        "\n",
        "print(\"\\nComparison:\")\n",
        "if accuracy_max_depth_3 > accuracy_full:\n",
        "    print(\"The tree with max_depth=3 performed better or equally well.\")\n",
        "elif accuracy_full > accuracy_max_depth_3:\n",
        "    print(\"The fully-grown tree performed better.\")\n",
        "else:\n",
        "    print(\"Both trees achieved the same accuracy.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7hRP648bAs4",
        "outputId": "0fe921c6-f3f3-4688-9d0e-3a2d16a7db35"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Decision Tree (max_depth=3): 1.00\n",
            "Accuracy of Fully-grown Decision Tree: 1.00\n",
            "\n",
            "Comparison:\n",
            "Both trees achieved the same accuracy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 8: Write a Python program to:\n",
        "- Load the Boston Housing Dataset\n",
        "- Train a Decision Tree Regressor\n",
        "- Print the Mean Squared Error (MSE) and feature importances"
      ],
      "metadata": {
        "id": "7IWMImiXbLuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing # Changed from load_boston\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 1. Load the California Housing Dataset (as Boston is deprecated)\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 2. Train a Decision Tree Regressor\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "dt_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = dt_regressor.predict(X_test)\n",
        "\n",
        "# 3. Print the Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "\n",
        "# 4. Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(housing.feature_names, dt_regressor.feature_importances_):\n",
        "    print(f\"  {feature}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9ry2EOubOgr",
        "outputId": "edf8de11-799c-48b5-f7ad-c02f0989082e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.53\n",
            "\n",
            "Feature Importances:\n",
            "  MedInc: 0.5235\n",
            "  HouseAge: 0.0521\n",
            "  AveRooms: 0.0494\n",
            "  AveBedrms: 0.0250\n",
            "  Population: 0.0322\n",
            "  AveOccup: 0.1390\n",
            "  Latitude: 0.0900\n",
            "  Longitude: 0.0888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 9: Write a Python program to:\n",
        "- Load the Iris Dataset\n",
        "- Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV\n",
        "- Print the best parameters and the resulting model accuracy"
      ],
      "metadata": {
        "id": "M7Mni23cbfFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "# We'll use a test set to evaluate the final best model\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'max_depth': [1, 2, 3, 4, 5, None], # None means full depth\n",
        "    'min_samples_split': [2, 5, 10, 15]\n",
        "}\n",
        "\n",
        "# Initialize the Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# 2. Tune the Decision Tree's max_depth and min_samples_split using GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=dt_classifier,\n",
        "                           param_grid=param_grid,\n",
        "                           cv=5, # 5-fold cross-validation\n",
        "                           scoring='accuracy',\n",
        "                           n_jobs=-1, # Use all available cores\n",
        "                           verbose=1)\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 3. Print the best parameters and the resulting model accuracy\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# Get the best estimator (model) found by GridSearchCV\n",
        "best_dt_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the unseen test set using the best model\n",
        "y_pred_best = best_dt_model.predict(X_test)\n",
        "accuracy_best_model = accuracy_score(y_test, y_pred_best)\n",
        "print(f\"Accuracy of the best model on the test set: {accuracy_best_model:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CPcNUEmbjcz",
        "outputId": "a79aa4f7-e9e8-420b-be05-74e86de75a22"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 10}\n",
            "Accuracy of the best model on the test set: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 10: Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.\n",
        "\n",
        "Explain the step-by-step process you would follow to:\n",
        "- Handle the missing values\n",
        "- Encode the categorical features\n",
        "- Train a Decision Tree model\n",
        "- Tune its hyperparameters\n",
        "- Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting."
      ],
      "metadata": {
        "id": "y--8JigAbx8k"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3ff5176"
      },
      "source": [
        "As a data scientist predicting a disease for a healthcare company, using a Decision Tree model with a large dataset of mixed types and missing values, I would follow these steps:\n",
        "\n",
        "### Step-by-Step Process:\n",
        "\n",
        "#### 1. Data Understanding and Initial Exploration:\n",
        "*   **Load Data**: Load the dataset into a Pandas DataFrame.\n",
        "*   **Initial Inspection**: Check `df.info()`, `df.describe(include='all')` to get an overview of data types, non-null counts, and basic statistics.\n",
        "*   **Identify Target Variable**: Clearly define the target variable (e.g., 'Disease_Presence' - binary classification).\n",
        "*   **Feature Identification**: Distinguish between numerical, categorical, and potentially ordinal features.\n",
        "\n",
        "#### 2. Handle Missing Values:\n",
        "\n",
        "Missing values need careful handling as Decision Trees can sometimes handle them inherently (e.g., by creating surrogate splits), but often imputation or removal is better for overall model quality and interpretability.\n",
        "\n",
        "*   **Quantify Missingness**: Use `df.isnull().sum()` to identify columns with missing values and the extent of missingness.\n",
        "*   **Strategy Selection**: The approach depends on the feature type and the amount of missing data:\n",
        "    *   **Numerical Features**:\n",
        "        *   **Imputation**: For features with a moderate amount of missing data, I'd consider **mean, median, or mode imputation**. Median is often preferred over mean for skewed distributions.\n",
        "        *   **Advanced Imputation**: For more complex relationships, k-Nearest Neighbors (KNN) imputation or Multiple Imputation by Chained Equations (MICE) could be considered, though they are more computationally intensive.\n",
        "        *   **Removal**: If a numerical feature has a very high percentage of missing values (e.g., >70-80%), it might be best to remove the column entirely, assuming it doesn't hold crucial information.\n",
        "    *   **Categorical Features**:\n",
        "        *   **Mode Imputation**: Fill missing categorical values with the mode (most frequent category).\n",
        "        *   **'Missing' Category**: Create a new category called 'Missing' to explicitly indicate the absence of information, which can be useful if missingness itself is predictive.\n",
        "        *   **Removal**: Similar to numerical features, remove if nearly all values are missing.\n",
        "*   **Decision Tree Specific**: While some Decision Tree implementations can handle missing values by routing samples to the most common branch or by using surrogate splits, pre-imputing often leads to more robust models and easier integration with other pipeline steps.\n",
        "\n",
        "#### 3. Encode Categorical Features:\n",
        "\n",
        "Decision Trees from `sklearn` require numerical input. Categorical features must be converted.\n",
        "\n",
        "*   **Nominal Categorical Features** (no inherent order, e.g., 'Gender', 'Blood_Type'):\n",
        "    *   **One-Hot Encoding**: Create new binary columns for each category. This is generally preferred for Decision Trees as it doesn't imply an ordinal relationship. For categories with many unique values, this can lead to a high-dimensional sparse matrix, which might need dimensionality reduction if it becomes problematic.\n",
        "*   **Ordinal Categorical Features** (inherent order, e.g., 'Disease_Severity': Low, Medium, High):\n",
        "    *   **Ordinal Encoding**: Map categories to numerical values preserving their order (e.g., Low=0, Medium=1, High=2).\n",
        "*   **Binary Categorical Features** (e.g., 'Smoker': Yes/No):\n",
        "    *   **Label Encoding**: Convert to 0s and 1s.\n",
        "*   **Handling High Cardinality**: For categorical features with too many unique values, consider grouping rare categories or using more advanced encoding techniques like target encoding (with caution to prevent data leakage).\n",
        "\n",
        "#### 4. Train a Decision Tree Model:\n",
        "\n",
        "*   **Data Splitting**: Split the preprocessed dataset into training and testing sets (e.g., 70/30 or 80/20) using `train_test_split`. Ensure stratification if the target variable is imbalanced.\n",
        "*   **Model Initialization**: Initialize a `DecisionTreeClassifier` (for disease prediction, which is typically classification).\n",
        "    *   `from sklearn.tree import DecisionTreeClassifier`\n",
        "    *   `dt_model = DecisionTreeClassifier(random_state=42)` (set random_state for reproducibility).\n",
        "*   **Model Training**: Fit the model to the training data.\n",
        "    *   `dt_model.fit(X_train, y_train)`\n",
        "\n",
        "#### 5. Tune its Hyperparameters:\n",
        "\n",
        "Decision Trees are prone to overfitting, so tuning is crucial.\n",
        "\n",
        "*   **Identify Key Hyperparameters**: Relevant hyperparameters for Decision Trees include:\n",
        "    *   `max_depth`: The maximum depth of the tree (controls overfitting).\n",
        "    *   `min_samples_split`: The minimum number of samples required to split an internal node.\n",
        "    *   `min_samples_leaf`: The minimum number of samples required to be at a leaf node.\n",
        "    *   `criterion`: The function to measure the quality of a split (e.g., 'gini' or 'entropy').\n",
        "*   **Define a Search Space**: Create a dictionary specifying the ranges or lists of values to explore for each hyperparameter.\n",
        "    *   `param_grid = {'max_depth': [3, 5, 7, 10, None], 'min_samples_split': [2, 5, 10, 20], 'criterion': ['gini', 'entropy']}`\n",
        "*   **Tuning Method**: Employ `GridSearchCV` or `RandomizedSearchCV` for systematic hyperparameter tuning.\n",
        "    *   `from sklearn.model_selection import GridSearchCV`\n",
        "    *   `grid_search = GridSearchCV(estimator=dt_model, param_grid=param_grid, cv=5, scoring='roc_auc', n_jobs=-1, verbose=1)`\n",
        "    *   `grid_search.fit(X_train, y_train)`\n",
        "*   **Select Best Model**: Retrieve the best estimator found by the search.\n",
        "    *   `best_dt_model = grid_search.best_estimator_`\n",
        "    *   `print(f\"Best Parameters: {grid_search.best_params_}\")`\n",
        "\n",
        "#### 6. Evaluate its Performance:\n",
        "\n",
        "Evaluation is critical to understand the model's effectiveness on unseen data.\n",
        "\n",
        "*   **Predictions**: Make predictions on the test set using the `best_dt_model`.\n",
        "    *   `y_pred = best_dt_model.predict(X_test)`\n",
        "    *   `y_proba = best_dt_model.predict_proba(X_test)[:, 1]` (for probability-based metrics)\n",
        "*   **Classification Metrics**: Given a disease prediction scenario, the focus often shifts beyond just accuracy, especially if the disease prevalence is low (imbalanced dataset).\n",
        "    *   **Accuracy**: Overall correct predictions (`accuracy_score`).\n",
        "    *   **Precision**: Of all predicted positives, how many were actually positive.\n",
        "    *   **Recall (Sensitivity)**: Of all actual positives, how many did the model correctly identify (crucial for not missing sick patients).\n",
        "    *   **F1-Score**: Harmonic mean of precision and recall.\n",
        "    *   **ROC AUC**: Area Under the Receiver Operating Characteristic Curve (measures the trade-off between true positive rate and false positive rate). This is often a robust metric for imbalanced classification.\n",
        "    *   **Confusion Matrix**: Provides a detailed breakdown of True Positives, True Negatives, False Positives, and False Negatives.\n",
        "*   **Cross-Validation**: The `GridSearchCV` already incorporates cross-validation on the training set, but it's good practice to ensure the final model generalizes well to the completely unseen test set.\n",
        "\n",
        "### Business Value in Real-World Setting:\n",
        "\n",
        "A well-performing Decision Tree model in a healthcare setting could provide significant business value:\n",
        "\n",
        "1.  **Early Disease Detection**: The model can identify patients at high risk of having a disease even before symptoms become severe, allowing for earlier intervention and potentially more effective treatment.\n",
        "2.  **Resource Optimization**: By predicting disease likelihood, healthcare providers can allocate resources more efficiently, focusing diagnostic tests, specialist consultations, and preventative measures on those who need them most.\n",
        "3.  **Cost Reduction**: Early and accurate prediction can prevent the progression of diseases to more severe, expensive stages, leading to lower treatment costs for both patients and the healthcare system.\n",
        "4.  **Improved Patient Outcomes**: Timely diagnosis and treatment based on model predictions can lead to better health outcomes, improved quality of life for patients, and reduced mortality rates.\n",
        "5.  **Personalized Treatment Plans**: Understanding the factors (features) that contribute to the disease prediction (through feature importances) can help tailor personalized treatment and prevention plans for individuals.\n",
        "6.  **Screening Program Design**: The model can help in designing more effective and targeted screening programs by identifying demographic or clinical risk factors that the tree highlights.\n",
        "7.  **Diagnostic Aid**: For clinicians, the model can serve as an assistive tool, providing a data-driven second opinion or highlighting potential diagnoses that might otherwise be overlooked, especially in complex cases.\n",
        "8.  **Risk Stratification**: Patients can be stratified into different risk groups, enabling differentiated care pathways. High-risk patients could receive more intensive monitoring, while low-risk patients could be reassured or managed with less aggressive interventions."
      ]
    }
  ]
}